{"cells":[{"cell_type":"code","execution_count":null,"id":"dc4f2ceb-2431-473a-876c-260b553a6218","metadata":{"id":"dc4f2ceb-2431-473a-876c-260b553a6218"},"outputs":[],"source":["#testing Lincolnbeet on transfer learned weights of MW dataset with ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt) \n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --taks test --img 864 --batch 8 --conf 0.001 --iou 0.65 --device 0 --weights runs/train/transferlearn_linb_e300_bs8_ms_img864_/weights/best.pt --name MW3ms_linb_bs8_img864_ebest_"]},{"cell_type":"code","source":["#testing Lincolnbeet on transfer learned weights of MW dataset without ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt) \n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --taks test --img 864 --batch 8 --conf 0.001 --iou 0.65 --device 0 --weights runs/train/transferlearn_linb_e300_bs8_img864_/weights/best.pt --name MW3_linb_bs8_img864_ebest_"],"metadata":{"id":"lqo9bckyyhqS"},"id":"lqo9bckyyhqS","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f39b59db-9d16-4253-b540-f551cd95d891","metadata":{"id":"f39b59db-9d16-4253-b540-f551cd95d891","outputId":"cd0b3049-8187-4581-bb72-c13cadc23c28"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_linb_e300_bs8_ms_img864_coco_3/weights/best.pt'], data='data/custom_data.yaml', batch_size=8, img_size=864, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='COCO3ms_linb_img864_bs8_ebest_', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 1.12.0+cu116 CUDA:0 (NVIDIA TITAN RTX, 24212.375MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/jovyan/datasets/all_fields_lincolnbeet/test/labels.cache' \u001b[0m\n","               Class      Images      Labels           P           R      mAP@.5\n","                 all         883        7824       0.742        0.76       0.787       0.542\n","                weed         883        4555       0.668        0.69         0.7       0.387\n","                crop         883        3269       0.817       0.829       0.873       0.698\n","Speed: 5.1/0.8/5.9 ms inference/NMS/total per 864x864 image at batch-size 8\n","[[    0.73508     0.10417     0.78901]\n"," [   0.072231     0.81237     0.21099]\n"," [    0.19269     0.08346         nan]]\n","Results saved to runs/test/COCO3ms_linb_img864_bs8_ebest_3\n"]}],"source":["#testing Lincolnbeet on transfer learned weights of COCO with ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt) \n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 864 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_linb_e300_bs8_ms_img864_coco_/weights/best.pt --name COCO3ms_linb_img864_bs8_ebest_"]},{"cell_type":"code","source":["#testing Lincolnbeet on transfer learned weights of COCO without ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt) \n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 864 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_linb_e300_bs8_img864_coco_/weights/best.pt --name COCO3_linb_img864_bs8_ebest_"],"metadata":{"id":"jUp-v0_5y0zE"},"id":"jUp-v0_5y0zE","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"d9b998c2-69dd-4adf-bac3-c9c61c6b6dca","metadata":{"id":"d9b998c2-69dd-4adf-bac3-c9c61c6b6dca","outputId":"64adc5bf-2c9e-4fa4-fb16-b36871568588"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_grapes_e300_bs8_ms_img512_/weights/best.pt'], data='data/custom_data.yaml', batch_size=8, img_size=512, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='AG3ms_grapes_img512_bs8_ebest_', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 1.12.0+cu116 CUDA:0 (NVIDIA TITAN RTX, 24212.375MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/jovyan/datasets/grapes_right/test/labels.cache' images and\u001b[0m\n","               Class      Images      Labels           P           R      mAP@.5\n","                 all         834        1454        0.56       0.372       0.364       0.118\n","                crop         834        1454        0.56       0.372       0.364       0.118\n","Speed: 3.7/0.7/4.5 ms inference/NMS/total per 512x512 image at batch-size 8\n","[[        nan         nan         nan]\n"," [        nan         606         310]\n"," [        nan         848         nan]]\n","Results saved to runs/test/AG3ms_grapes_img512_bs8_ebest_2\n"]}],"source":["#testing Grapes on transfer learned weights of MW dataset with ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt) \n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e300_bs8_ms_img512_/weights/best.pt --name MW3ms_grapes_img512_bs8_ebest_"]},{"cell_type":"code","execution_count":null,"id":"b980c4b1-8736-4193-a155-1a09904f5560","metadata":{"id":"b980c4b1-8736-4193-a155-1a09904f5560","outputId":"0e58dffc-4209-4200-b570-ffe9362e2a7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_grapes_e300_bs8_ms_img512_/weights/epoch_024.pt'], data='data/custom_data.yaml', batch_size=8, img_size=512, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='AG25ms_grapes_img512_bs8_e25_', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 1.12.0+cu116 CUDA:0 (NVIDIA TITAN RTX, 24212.375MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/jovyan/datasets/grapes_right/test/labels.cache' images and\u001b[0m\n","               Class      Images      Labels           P           R      mAP@.5\n","                 all         834        1454       0.393       0.313       0.251      0.0699\n","                crop         834        1454       0.393       0.313       0.251      0.0699\n","Speed: 3.7/0.9/4.6 ms inference/NMS/total per 512x512 image at batch-size 8\n","[[        nan         nan         nan]\n"," [        nan         451         227]\n"," [        nan        1003         nan]]\n","Results saved to runs/test/AG25ms_grapes_img512_bs8_e25_2\n"]}],"source":["#testing Grapes on transfer learned weights of MW dataset without ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt)\n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e300_bs8_img512_/weights/best.pt --name MW3_grapes_img512_bs8_ebest_"]},{"cell_type":"code","source":["#testing Grapes on transfer learned weights of MW dataset with ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt)\n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e300_bs8_ms_img512_/weights/best.pt --name MW3ms_grapes_img512_bs8_ebest_"],"metadata":{"id":"Jt6f-62az2kx"},"id":"Jt6f-62az2kx","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4c9c015f-cb4f-46a0-ae21-57ada0d85553","metadata":{"id":"4c9c015f-cb4f-46a0-ae21-57ada0d85553","outputId":"65d3f5e5-7a19-467a-fbcc-cfa2768db87a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_grapes_e300_bs8_ms_img512_coco_/weights/epoch_074.pt'], data='data/custom_data.yaml', batch_size=8, img_size=512, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='COCO3ms_grapes_img512_bs8_e75_', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 1.12.0+cu116 CUDA:0 (NVIDIA TITAN RTX, 24212.375MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/jovyan/datasets/grapes_right/test/labels.cache' images and\u001b[0m\n","               Class      Images      Labels           P           R      mAP@.5\n","                 all         834        1454       0.128        0.16      0.0505      0.0144\n","                crop         834        1454       0.128        0.16      0.0505      0.0144\n","Speed: 5.1/2.3/7.3 ms inference/NMS/total per 512x512 image at batch-size 8\n","[[        nan         nan         nan]\n"," [        nan         nan         nan]\n"," [        nan        1454         nan]]\n","Results saved to runs/test/COCO3ms_grapes_img512_bs8_e75_\n"]}],"source":["#testing Grapes on transfer learned weights of COCO dataset with ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt)\n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e300_bs8_ms_img512_coco_/weights/best.pt --name COCO3ms_grapes_img512_bs8_ebest_"]},{"cell_type":"code","execution_count":null,"id":"89d79db9-3e58-4884-b482-3eb0b213df58","metadata":{"id":"89d79db9-3e58-4884-b482-3eb0b213df58","outputId":"66ed7d4e-4089-4bef-a259-0b6b63826eae"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_grapes_e300_bs8_img512_coco_/weights/epoch_074.pt'], data='data/custom_data.yaml', batch_size=8, img_size=512, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='COCO3_grapes_img512_bs8_e75_', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 1.12.0+cu116 CUDA:0 (NVIDIA TITAN RTX, 24212.375MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/jovyan/datasets/grapes_right/test/labels.cache' images and\u001b[0m\n","               Class      Images      Labels           P           R      mAP@.5\n","                 all         834        1454       0.433       0.309       0.277      0.0879\n","                crop         834        1454       0.433       0.309       0.277      0.0879\n","Speed: 3.8/0.9/4.7 ms inference/NMS/total per 512x512 image at batch-size 8\n","[[        nan         nan         nan]\n"," [        nan         362         122]\n"," [        nan        1092         nan]]\n","Results saved to runs/test/COCO3_grapes_img512_bs8_e75_\n"]}],"source":["#testing Grapes on transfer learned weights of COCO dataset without ms and 300 epochs for the best epoch (best.pt), epoch 0 (epoch_000.pt), epoch 50 (epoch_050.pt), \n","#epoch 100 (epoch_100.pt), epoch 150 (epoch_150.pt), epoch 200 (epoch_250.pt), epoch 300 (epoch_300.pt)\n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e300_bs8_img512_coco_/weights/best.pt --name COCO3_grapes_img512_bs8_e75_"]},{"cell_type":"code","execution_count":null,"id":"8bc3d352-af45-4fec-95ce-c716f0c5dcce","metadata":{"id":"8bc3d352-af45-4fec-95ce-c716f0c5dcce","outputId":"fc947d9f-b2d2-43fb-8966-819485330de3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_grapes_e300_bs8_img512_3/weights/epoch_024.pt'], data='data/custom_data.yaml', batch_size=8, img_size=512, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='AG3_grapes_img512_bs8_e25_', exist_ok=False, no_trace=False, v5_metric=False)\n","YOLOR ðŸš€ v0.1-122-g3b41c2c torch 1.12.0+cu116 CUDA:0 (NVIDIA TITAN RTX, 24212.375MB)\n","\n","Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n","Model Summary: 314 layers, 36487166 parameters, 6194944 gradients\n"," Convert model to Traced-model... \n"," traced_script_module saved! \n"," model is traced! \n","\n","/opt/conda/lib/python3.10/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","\u001b[34m\u001b[1mtest: \u001b[0mScanning '/home/jovyan/datasets/grapes_right/test/labels.cache' images and\u001b[0m\n","               Class      Images      Labels           P           R      mAP@.5\n","                 all         834        1454       0.518       0.343       0.341       0.105\n","                crop         834        1454       0.518       0.343       0.341       0.105\n","Speed: 5.3/2.1/7.3 ms inference/NMS/total per 512x512 image at batch-size 8\n","[[        nan         nan         nan]\n"," [        nan         587         379]\n"," [        nan         867         nan]]\n","Results saved to runs/test/AG3_grapes_img512_bs8_e25_\n"]}],"source":["#testing grapes on transfer learned weights AG without ms epoch 0\n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e300_bs8_img512_3/weights/epoch_024.pt --name AG3_grapes_img512_bs8_e25_"]},{"cell_type":"code","execution_count":null,"id":"1d7d61fb-7e8b-4639-a41c-7fa96c019935","metadata":{"id":"1d7d61fb-7e8b-4639-a41c-7fa96c019935","outputId":"5e5cb2c7-6944-4de3-debf-4ce92beeb492"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/jovyan/yolov7\n","Namespace(weights=['runs/train/transferlearn_grapes_e10_bs8_img512_/weights/epoch_009.pt'], data='data/custom_data.yaml', batch_size=8, img_size=512, conf_thres=0.001, iou_thres=0.65, task='test', device='0', single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project='runs/test', name='AG10_grapes_img512_bs8_e9_', exist_ok=False, no_trace=False, v5_metric=False)\n","Traceback (most recent call last):\n","  File \"/home/jovyan/yolov7/test.py\", line 328, in <module>\n","    test(opt.data,\n","  File \"/home/jovyan/yolov7/test.py\", line 51, in test\n","    device = select_device(opt.device, batch_size=batch_size)\n","  File \"/home/jovyan/yolov7/utils/torch_utils.py\", line 71, in select_device\n","    assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability\n","AssertionError: CUDA unavailable, invalid device 0 requested\n"]}],"source":["#testing grapes on transfer learned weights AG without ms epoch 0\n","%cd /home/jovyan/yolov7\n","!python test.py --data data/custom_data.yaml --img 512 --batch 8 --conf 0.001 --iou 0.65 --device 0 --task test --weights runs/train/transferlearn_grapes_e10_bs8_img512_/weights/epoch_009.pt --name AG10_grapes_img512_bs8_e9_"]},{"cell_type":"code","execution_count":null,"id":"0df88bff-8484-4650-98df-e78f6750581c","metadata":{"id":"0df88bff-8484-4650-98df-e78f6750581c"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}